{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f:\\\\Files\\\\DS&ML\\\\FraudGuard\\\\notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f:\\\\Files\\\\DS&ML\\\\FraudGuard'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('../')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class ModelEvaluationConfig:\n",
    "    root_dir: Path\n",
    "    test_path: Path\n",
    "    preprocess_path: Path\n",
    "    model_path: Path\n",
    "    metrics_path: str\n",
    "    target_column: str\n",
    "    cm_path: Path\n",
    "    roc_path: Path\n",
    "    mlflow_username: str\n",
    "    mlflow_password: str\n",
    "    experiment_name: str\n",
    "    tracking_uri: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FraudGuard.constants import *\n",
    "from FraudGuard.utils.helpers import *\n",
    "from FraudGuard.utils.exceptions import *\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self,\n",
    "                 config_filepath = CONFIG_PATH,\n",
    "                 params_filepath = PARAMS_PATH,\n",
    "                 schema_filepath = SCHEMA_PATH):\n",
    "        \n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "        config = self.config.model_evaluation\n",
    "        schema = self.schema.target_column\n",
    "        mlflow_params = self.params.mlflow\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_evaluation_config = ModelEvaluationConfig(\n",
    "            root_dir= config.root_dir,\n",
    "            test_path= config.test_path,\n",
    "            model_path= config.model_path,\n",
    "            preprocess_path= config.preprocess_path,\n",
    "            metrics_path= config.metrics_path,\n",
    "            target_column= schema.name,\n",
    "            cm_path= config.cm_path,\n",
    "            roc_path= config.roc_path,\n",
    "            mlflow_username= mlflow_params.mlflow_username,\n",
    "            mlflow_password= mlflow_params.mlflow_password,\n",
    "            experiment_name= mlflow_params.experiment_name,\n",
    "            tracking_uri= mlflow_params.tracking_uri\n",
    "        )\n",
    "\n",
    "        return model_evaluation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import mlflow\n",
    "import dagshub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_curve, auc, confusion_matrix\n",
    ")\n",
    "\n",
    "from FraudGuard import logger\n",
    "from FraudGuard.utils.helpers import save_json\n",
    "from FraudGuard.entity.config_entity import ModelEvaluationConfig\n",
    "\n",
    "\n",
    "class ModelEvaluation:\n",
    "    def __init__(self, config: ModelEvaluationConfig):\n",
    "        self.config = config\n",
    "\n",
    "        # Set MLflow credentials\n",
    "        os.environ[\"MLFLOW_TRACKING_USERNAME\"] = self.config.mlflow_username\n",
    "        os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = self.config.mlflow_password\n",
    "\n",
    "        # Initialize Dagshub\n",
    "        dagshub.init(\n",
    "            repo_owner=\"JavithNaseem-J\",\n",
    "            repo_name=\"FraudGuard.mlflow\", \n",
    "            mlflow=True\n",
    "        )\n",
    "\n",
    "        mlflow.set_tracking_uri(self.config.tracking_uri)\n",
    "        mlflow.set_experiment(\"Fraud-Detection\")\n",
    "\n",
    "    def evaluation(self):\n",
    "        # Validate paths\n",
    "        if not os.path.exists(self.config.test_path):\n",
    "            raise FileNotFoundError(f\"Test data not found: {self.config.test_path}\")\n",
    "        if not os.path.exists(self.config.model_path):\n",
    "            raise FileNotFoundError(f\"Model not found: {self.config.model_path}\")\n",
    "        if not os.path.exists(self.config.preprocess_path):\n",
    "            raise FileNotFoundError(f\"Preprocessor not found: {self.config.preprocess_path}\")\n",
    "\n",
    "        # Load artifacts\n",
    "        test_df = pd.read_csv(self.config.test_path)\n",
    "        model = joblib.load(self.config.model_path)\n",
    "        preprocessor = joblib.load(self.config.preprocess_path)\n",
    "\n",
    "        # Split features/target\n",
    "        target_column = self.config.target_column\n",
    "        X_test = test_df.drop(columns=[target_column])\n",
    "        y_test = test_df[target_column]\n",
    "        X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "        # Make predictions\n",
    "        preds = model.predict(X_test_transformed)\n",
    "\n",
    "        # Metrics\n",
    "        metrics = {\n",
    "            \"accuracy\": accuracy_score(y_test, preds),\n",
    "            \"precision_weighted\": precision_score(y_test, preds, average=\"weighted\"),\n",
    "            \"recall_weighted\": recall_score(y_test, preds, average=\"weighted\"),\n",
    "            \"f1_weighted\": f1_score(y_test, preds, average=\"weighted\"),\n",
    "            \"precision_macro\": precision_score(y_test, preds, average=\"macro\"),\n",
    "            \"recall_macro\": recall_score(y_test, preds, average=\"macro\"),\n",
    "            \"f1_macro\": f1_score(y_test, preds, average=\"macro\"),\n",
    "        }\n",
    "\n",
    "        if proba is not None:\n",
    "            fpr, tpr, _ = roc_curve(y_test, proba)\n",
    "            metrics[\"auc\"] = auc(fpr, tpr)\n",
    "\n",
    "        # Save metrics to JSON\n",
    "        os.makedirs(self.config.root_dir, exist_ok=True)\n",
    "        save_json(path=Path(self.config.metrics_path), data=metrics)\n",
    "\n",
    "        try:\n",
    "            with mlflow.start_run(run_name=\"Model Evaluation\"):\n",
    "                mlflow.log_metrics({k: float(v) for k, v in metrics.items()})\n",
    "                mlflow.set_tag(\"stage\", \"evaluation\")\n",
    "\n",
    "                # Log artifacts\n",
    "                mlflow.log_artifact(self.config.metrics_path)\n",
    "                mlflow.log_artifact(self.config.model_path)\n",
    "                mlflow.log_artifact(self.config.preprocess_path)\n",
    "\n",
    "                # Confusion Matrix\n",
    "                cm = confusion_matrix(y_test, preds)\n",
    "                plt.figure(figsize=(6, 4))\n",
    "                sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "                plt.title(\"Confusion Matrix\")\n",
    "                plt.xlabel(\"Predicted\")\n",
    "                plt.ylabel(\"Actual\")\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(self.config.cm_path)\n",
    "                plt.close()\n",
    "                mlflow.log_artifact(self.config.cm_path)\n",
    "\n",
    "                # ROC Curve\n",
    "                if proba is not None:\n",
    "                    plt.figure(figsize=(6, 4))\n",
    "                    plt.plot(fpr, tpr, label=f\"AUC = {metrics['auc']:.2f}\")\n",
    "                    plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
    "                    plt.xlabel(\"False Positive Rate\")\n",
    "                    plt.ylabel(\"True Positive Rate\")\n",
    "                    plt.title(\"ROC Curve\")\n",
    "                    plt.legend()\n",
    "                    plt.tight_layout()\n",
    "                    plt.savefig(self.config.roc_path, bbox_inches=\"tight\")\n",
    "                    plt.close()\n",
    "                    mlflow.log_artifact(self.config.roc_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during evaluation and logging: {e}\")\n",
    "            raise\n",
    "\n",
    "        logger.info(\"✅ Model evaluation complete. Metrics and plots logged.\")\n",
    "        return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-13 12:46:44,550: INFO: helpers: yaml file: config_file\\config.yaml loaded successfully]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-13 12:46:44,551: INFO: helpers: yaml file: config_file\\schema.yaml loaded successfully]\n",
      "[2025-06-13 12:46:44,564: INFO: helpers: yaml file: config_file\\params.yaml loaded successfully]\n",
      "[2025-06-13 12:46:44,567: INFO: helpers: created directory at: artifacts]\n",
      "[2025-06-13 12:46:44,568: INFO: helpers: created directory at: artifacts/model_evaluation]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-13 12:46:45,245: INFO: helpers: Repository initialized!]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/13 12:46:46 INFO mlflow.tracking.fluent: Experiment with name 'Fraud-Detection' does not exist. Creating a new experiment.\n",
      "c:\\Users\\Javith Naseem\\.conda\\envs\\FraudGraud\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Javith Naseem\\.conda\\envs\\FraudGraud\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-13 12:46:54,519: INFO: helpers: json file saved at: artifacts\\model_evaluation\\metrics.json]\n",
      "🏃 View run Model Evaluation at: https://dagshub.com/JavithNaseem-J/FraudGuard.mlflow/#/experiments/0/runs/103626c75c1e463691d1a0b32295318a\n",
      "🧪 View experiment at: https://dagshub.com/JavithNaseem-J/FraudGuard.mlflow/#/experiments/0\n",
      "[2025-06-13 12:47:02,840: INFO: 2921192990: ✅ Model evaluation complete. Metrics and plots logged.]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_evaluation_config = config.get_model_evaluation_config()\n",
    "    model_evaluation = ModelEvaluation(config=model_evaluation_config)\n",
    "    model_evaluation.evaluation()\n",
    "except Exception as e:\n",
    "    raise CustomException(str(e), sys)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
