{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f:\\\\Files\\\\DS&ML\\\\FraudGuard\\\\notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f:\\\\Files\\\\DS&ML\\\\FraudGuard'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('../')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class ModelEvaluationConfig:\n",
    "    root_dir: Path\n",
    "    test_path: Path\n",
    "    preprocess_path: Path\n",
    "    model_path: Path\n",
    "    metrics_path: str\n",
    "    target_column: str\n",
    "    cm_path: Path\n",
    "    roc_path: Path\n",
    "    mlflow_username: str\n",
    "    mlflow_password: str\n",
    "    experiment_name: str\n",
    "    tracking_uri: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FraudGuard.constants import *\n",
    "from FraudGuard.utils.helpers import *\n",
    "from FraudGuard.utils.exceptions import *\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self,\n",
    "                 config_filepath = CONFIG_PATH,\n",
    "                 params_filepath = PARAMS_PATH,\n",
    "                 schema_filepath = SCHEMA_PATH):\n",
    "        \n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "        config = self.config.model_evaluation\n",
    "        schema = self.schema.target_column\n",
    "        mlflow_params = self.params.mlflow\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_evaluation_config = ModelEvaluationConfig(\n",
    "            root_dir= config.root_dir,\n",
    "            test_path= config.test_path,\n",
    "            model_path= config.model_path,\n",
    "            preprocess_path= config.preprocess_path,\n",
    "            metrics_path= config.metrics_path,\n",
    "            target_column= schema.name,\n",
    "            cm_path= config.cm_path,\n",
    "            roc_path= config.roc_path,\n",
    "            mlflow_username= mlflow_params.mlflow_username,\n",
    "            mlflow_password= mlflow_params.mlflow_password,\n",
    "            experiment_name= mlflow_params.experiment_name,\n",
    "            tracking_uri= mlflow_params.tracking_uri\n",
    "        )\n",
    "\n",
    "        return model_evaluation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_evaluation.py\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_curve, auc, confusion_matrix\n",
    ")\n",
    "from FraudGuard import logger\n",
    "from FraudGuard.utils.helpers import save_json\n",
    "from FraudGuard.entity.config_entity import ModelEvaluationConfig\n",
    "\n",
    "\n",
    "class ModelEvaluation:\n",
    "    def __init__(self, config: ModelEvaluationConfig):\n",
    "        self.config = config\n",
    "        os.environ[\"MLFLOW_TRACKING_USERNAME\"] = self.config.mlflow_username\n",
    "        os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = self.config.mlflow_password\n",
    "\n",
    "    def evaluation(self):\n",
    "        # Validate input paths\n",
    "        if not os.path.exists(self.config.test_path):\n",
    "            raise FileNotFoundError(f\"Test data not found: {self.config.test_path}\")\n",
    "        if not os.path.exists(self.config.model_path):\n",
    "            raise FileNotFoundError(f\"Model not found: {self.config.model_path}\")\n",
    "        if not os.path.exists(self.config.preprocess_path):\n",
    "            raise FileNotFoundError(f\"Preprocessor not found: {self.config.preprocess_path}\")\n",
    "        \n",
    "        mlflow.set_tracking_uri(\"https://dagshub.com/JavithNaseem-J/FraudGuard.mlflow\")\n",
    "        mlflow.set_experiment(\"Fraud-Detection\")\n",
    "\n",
    "        test_df = pd.read_csv(self.config.test_path)\n",
    "        model = joblib.load(self.config.model_path)\n",
    "        preprocessor = joblib.load(self.config.preprocess_path)\n",
    "\n",
    "        target_column = self.config.target_column\n",
    "        X_test = test_df.drop(columns=[target_column])\n",
    "        y_test = test_df[target_column]\n",
    "        X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "        # Predictions\n",
    "        preds = model.predict(X_test_transformed)\n",
    "        proba = model.predict_proba(X_test_transformed)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "        # Metrics\n",
    "        metrics = {\n",
    "            \"accuracy\": accuracy_score(y_test, preds),\n",
    "            \"precision_weighted\": precision_score(y_test, preds, average=\"weighted\"),\n",
    "            \"recall_weighted\": recall_score(y_test, preds, average=\"weighted\"),\n",
    "            \"f1_weighted\": f1_score(y_test, preds, average=\"weighted\"),\n",
    "            \"precision_macro\": precision_score(y_test, preds, average=\"macro\"),\n",
    "            \"recall_macro\": recall_score(y_test, preds, average=\"macro\"),\n",
    "            \"f1_macro\": f1_score(y_test, preds, average=\"macro\"),\n",
    "        }\n",
    "\n",
    "        if proba is not None:\n",
    "            fpr, tpr, _ = roc_curve(y_test, proba)\n",
    "            metrics[\"auc\"] = auc(fpr, tpr)\n",
    "\n",
    "        # Save metrics JSON\n",
    "        os.makedirs(self.config.root_dir, exist_ok=True)\n",
    "        save_json(path=Path(self.config.metrics_path), data=metrics)\n",
    "\n",
    "        with mlflow.start_run(run_name=\"Model Evaluation\"):\n",
    "            mlflow.log_metrics(metrics)\n",
    "            mlflow.set_tag(\"stage\", \"evaluation\")\n",
    "            mlflow.log_artifact(Path(self.config.metrics_path))\n",
    "\n",
    "            # Confusion Matrix\n",
    "            cm = confusion_matrix(y_test, preds)\n",
    "            plt.figure(figsize=(6, 4))\n",
    "            sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "            plt.title(\"Confusion Matrix\")\n",
    "            plt.xlabel(\"Predicted\")\n",
    "            plt.ylabel(\"Actual\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(Path(self.config.cm_path))\n",
    "            plt.close()\n",
    "            mlflow.log_artifact(Path(self.config.cm_path))\n",
    "\n",
    "            if proba is not None:\n",
    "                plt.figure(figsize=(6, 4))\n",
    "                plt.plot(fpr, tpr, label=f\"AUC = {metrics['auc']:.2f}\")\n",
    "                plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
    "                plt.xlabel(\"False Positive Rate\")\n",
    "                plt.ylabel(\"True Positive Rate\")\n",
    "                plt.title(\"ROC Curve\")\n",
    "                plt.legend()\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(Path(self.config.roc_path), bbox_inches=\"tight\")\n",
    "                plt.close()\n",
    "                mlflow.log_artifact(Path(self.config.roc_path))\n",
    "\n",
    "        logger.info(\"Model evaluation complete. Metrics and plots logged.\")\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-27 12:39:52,173: INFO: helpers: yaml file: config_file\\config.yaml loaded successfully]\n",
      "[2025-05-27 12:39:52,179: INFO: helpers: yaml file: config_file\\schema.yaml loaded successfully]\n",
      "[2025-05-27 12:39:52,186: INFO: helpers: yaml file: config_file\\params.yaml loaded successfully]\n",
      "[2025-05-27 12:39:52,186: INFO: helpers: created directory at: artifacts]\n",
      "[2025-05-27 12:39:52,186: INFO: helpers: created directory at: artifacts/model_evaluation]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\ProgramFiles\\anaconda3\\envs\\fraud-detection\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-27 12:40:00,967: INFO: helpers: json file saved at: artifacts\\model_evaluation\\metrics.json]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\ProgramFiles\\anaconda3\\envs\\fraud-detection\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run Model Evaluation at: https://dagshub.com/JavithNaseem-J/FraudGuard.mlflow/#/experiments/1/runs/a5a0912ecf874b3ca0fc456d07ca3354\n",
      "🧪 View experiment at: https://dagshub.com/JavithNaseem-J/FraudGuard.mlflow/#/experiments/1\n",
      "[2025-05-27 12:40:08,738: INFO: 2379928482: Model evaluation complete. Metrics and plots logged.]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_evaluation_config = config.get_model_evaluation_config()\n",
    "    model_evaluation = ModelEvaluation(config=model_evaluation_config)\n",
    "    model_evaluation.evaluation()\n",
    "except Exception as e:\n",
    "    raise CustomException(str(e), sys)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fraud-detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
